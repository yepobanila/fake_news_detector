{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb486f3",
   "metadata": {},
   "source": [
    "# 1) Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822275bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cpu\n",
      "Requirement already satisfied: torch in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (2.2.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73572c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (4.42.3)\n",
      "Requirement already satisfied: pandas in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (1.0.2)\n",
      "Requirement already satisfied: flask in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (1.1.2)\n",
      "Requirement already satisfied: streamlit in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (1.41.1)\n",
      "Requirement already satisfied: sentencepiece in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (0.2.0)\n",
      "Requirement already satisfied: shap in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (0.47.1)\n",
      "Requirement already satisfied: filelock in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from flask) (2.0.3)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from flask) (2.11.3)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from flask) (2.0.1)\n",
      "Requirement already satisfied: click>=5.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from flask) (8.0.4)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (5.5.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (4.2.2)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (9.0.1)\n",
      "Requirement already satisfied: protobuf<6,>=3.20 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (5.29.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (19.0.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (13.9.4)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (9.0.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (4.11.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (3.1.30)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from streamlit) (6.1)\n",
      "Requirement already satisfied: slicer==0.0.8 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from shap) (0.60.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (4.4.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from altair<6,>=4.0->streamlit) (1.25.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.10)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from Jinja2>=2.10.1->flask) (2.0.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from numba>=0.54->shap) (0.43.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from rich<14,>=10.14.0->streamlit) (2.19.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/yy/opt/anaconda3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers pandas scikit-learn flask streamlit sentencepiece shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f021a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yy/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/Users/yy/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed997f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS backend for Apple Silicon GPU (M1/M2)\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS backend for Apple Silicon GPU (M1/M2)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaec35b",
   "metadata": {},
   "source": [
    "# 2) Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a796b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1d/48rxts2n05v5ckcm452mg19w0000gn/T/ipykernel_26767/1935356005.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_fake_selected['content'] = news_fake_selected['content'].fillna(news_fake_selected['title'])\n",
      "/var/folders/1d/48rxts2n05v5ckcm452mg19w0000gn/T/ipykernel_26767/1935356005.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  news_real_selected['content'] = news_real_selected['content'].fillna(news_real_selected['title'])\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "claim_real = pd.read_csv('ClaimRealCOVID-19.csv')\n",
    "news_fake = pd.read_csv('NewsFakeCOVID-19.csv')\n",
    "news_real = pd.read_csv('NewsRealCOVID-19.csv')\n",
    "\n",
    "# Add labels\n",
    "claim_real['label'] = 1\n",
    "news_real['label'] = 1\n",
    "news_fake['label'] = 0\n",
    "\n",
    "# Select relevant columns\n",
    "claim_real_selected = claim_real[['title', 'label']]\n",
    "news_real_selected = news_real[['title', 'content', 'label']]\n",
    "news_fake_selected = news_fake[['title', 'content', 'label']]\n",
    "\n",
    "# Fill missing content\n",
    "news_fake_selected['content'] = news_fake_selected['content'].fillna(news_fake_selected['title'])\n",
    "news_real_selected['content'] = news_real_selected['content'].fillna(news_real_selected['title'])\n",
    "\n",
    "# Combine all datasets\n",
    "combined_df = pd.concat([claim_real_selected, news_real_selected, news_fake_selected], ignore_index=True)\n",
    "combined_df['content'] = combined_df['content'].fillna('')\n",
    "combined_df['content'] = combined_df['content'].astype(str)\n",
    "\n",
    "# Clean Text Function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "combined_df['cleaned_content'] = combined_df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba0f0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate real and fake news\n",
    "real_news = combined_df[combined_df['label'] == 1]\n",
    "fake_news = combined_df[combined_df['label'] == 0]\n",
    "\n",
    "# Upsample fake news to match real news count\n",
    "fake_news_upsampled = resample(fake_news, replace=True, n_samples=len(real_news), random_state=42)\n",
    "\n",
    "# Combine real and upsampled fake news\n",
    "balanced_df = pd.concat([real_news, fake_news_upsampled], ignore_index=True)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save balanced dataset\n",
    "balanced_df.to_csv('balanced_dataset.csv', index=False)\n",
    "\n",
    "# Reload dataset\n",
    "combined_df = pd.read_csv('balanced_dataset.csv')\n",
    "combined_df['cleaned_content'] = combined_df['cleaned_content'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11091076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_and_pad(texts, tokenizer, max_length=128):\n",
    "    encoding = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding['input_ids'], encoding['attention_mask']\n",
    "\n",
    "def prepare_dataloader(input_ids, attention_mask, labels, batch_size=4):\n",
    "    dataset = TensorDataset(input_ids, attention_mask, torch.tensor(labels.values))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dead641",
   "metadata": {},
   "source": [
    "# 3) Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25e351bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit training to 500 samples for testing\n",
    "X = combined_df['cleaned_content'][:500]\n",
    "y = combined_df['label'][:500]\n",
    "\n",
    "# Split dataset for training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert data to list\n",
    "X_train_list = X_train.tolist()\n",
    "X_val_list = X_val.tolist()\n",
    "\n",
    "# Tokenize\n",
    "X_train_ids, X_train_mask = tokenize_and_pad(X_train_list, bert_tokenizer)\n",
    "X_val_ids, X_val_mask = tokenize_and_pad(X_val_list, bert_tokenizer)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = prepare_dataloader(X_train_ids, X_train_mask, y_train)\n",
    "val_dataloader = prepare_dataloader(X_val_ids, X_val_mask, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "227b894f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, epochs=1, learning_rate=2e-5):\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            inputs, masks, labels = [b.to(device) for b in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids=inputs, attention_mask=masks, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {avg_train_loss}')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faacc15",
   "metadata": {},
   "source": [
    "### Training BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30aeccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.2979233482480049\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "trained_bert = train_model(bert_model, train_dataloader, epochs=1)\n",
    "torch.save(trained_bert.state_dict(), 'trained_bert_model.pth')\n",
    "del bert_model\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe844ca",
   "metadata": {},
   "source": [
    "### Training ALBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25ad9043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.47211181934922936\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "# Load ALBERT model\n",
    "albert_model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "albert_model.to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_albert = train_model(albert_model, train_dataloader, epochs=1)\n",
    "torch.save(trained_albert.state_dict(), 'trained_albert_model.pth')\n",
    "del albert_model\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa71253",
   "metadata": {},
   "source": [
    "### Training RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552542e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.4103435812331736\n"
     ]
    }
   ],
   "source": [
    "# Load RoBERTa model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "roberta_model.to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_roberta = train_model(roberta_model, train_dataloader, epochs=1)\n",
    "torch.save(trained_roberta.state_dict(), 'trained_roberta_model.pth')\n",
    "del roberta_model\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0455b338",
   "metadata": {},
   "source": [
    "# 4) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4496e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from transformers import BertForSequenceClassification, AlbertForSequenceClassification, RobertaForSequenceClassification\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_dataloader):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            inputs, masks, labels = [b.to(device) for b in batch]\n",
    "            \n",
    "            outputs = model(input_ids=inputs, attention_mask=masks)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(true_labels, predictions, target_names=[\"Fake\", \"Real\"])\n",
    "    print(report)\n",
    "    \n",
    "    # Return scores for comparison\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions)\n",
    "    recall = recall_score(true_labels, predictions)\n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3145ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.96      1.00      0.98        48\n",
      "        Real       1.00      0.96      0.98        52\n",
      "\n",
      "    accuracy                           0.98       100\n",
      "   macro avg       0.98      0.98      0.98       100\n",
      "weighted avg       0.98      0.98      0.98       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the trained BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "bert_model.load_state_dict(torch.load('trained_bert_model.pth'))\n",
    "\n",
    "# Evaluate BERT model\n",
    "bert_scores = evaluate_model(bert_model, val_dataloader)\n",
    "del bert_model\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53812bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.71      1.00      0.83        48\n",
      "        Real       1.00      0.62      0.76        52\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.85      0.81      0.79       100\n",
      "weighted avg       0.86      0.80      0.79       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the trained ALBERT model\n",
    "albert_model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)\n",
    "albert_model.load_state_dict(torch.load('trained_albert_model.pth'))\n",
    "\n",
    "# Evaluate ALBERT model\n",
    "albert_scores = evaluate_model(albert_model, val_dataloader)\n",
    "del albert_model\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc3b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.92      1.00      0.96        48\n",
      "        Real       1.00      0.92      0.96        52\n",
      "\n",
      "    accuracy                           0.96       100\n",
      "   macro avg       0.96      0.96      0.96       100\n",
      "weighted avg       0.96      0.96      0.96       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the trained RoBERTa model\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
    "roberta_model.load_state_dict(torch.load('trained_roberta_model.pth'))\n",
    "\n",
    "# Evaluate RoBERTa model\n",
    "roberta_scores = evaluate_model(roberta_model, val_dataloader)\n",
    "del roberta_model\n",
    "torch.mps.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e038434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model  Accuracy  Precision    Recall  F1 Score\n",
      "0     BERT      0.98        1.0  0.961538  0.980392\n",
      "1   ALBERT      0.80        1.0  0.615385  0.761905\n",
      "2  RoBERTa      0.96        1.0  0.923077  0.960000\n"
     ]
    }
   ],
   "source": [
    "# Combine scores for comparison\n",
    "import pandas as pd\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    \"Model\": [\"BERT\", \"ALBERT\", \"RoBERTa\"],\n",
    "    \"Accuracy\": [bert_scores[0], albert_scores[0], roberta_scores[0]],\n",
    "    \"Precision\": [bert_scores[1], albert_scores[1], roberta_scores[1]],\n",
    "    \"Recall\": [bert_scores[2], albert_scores[2], roberta_scores[2]],\n",
    "    \"F1 Score\": [bert_scores[3], albert_scores[3], roberta_scores[3]]\n",
    "})\n",
    "\n",
    "print(model_comparison)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
